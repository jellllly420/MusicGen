{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84f4c092e5744ecb9a81f4e005028ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_start_token_id: 2048\n",
      "MusicgenConfig {\n",
      "  \"architectures\": [\n",
      "    \"MusicgenForConditionalGeneration\"\n",
      "  ],\n",
      "  \"audio_encoder\": {\n",
      "    \"_name_or_path\": \"facebook/encodec_32khz\",\n",
      "    \"architectures\": [\n",
      "      \"EncodecModel\"\n",
      "    ],\n",
      "    \"audio_channels\": 1,\n",
      "    \"chunk_length_s\": null,\n",
      "    \"codebook_dim\": 128,\n",
      "    \"codebook_size\": 2048,\n",
      "    \"compress\": 2,\n",
      "    \"dilation_growth_rate\": 2,\n",
      "    \"hidden_size\": 128,\n",
      "    \"kernel_size\": 7,\n",
      "    \"last_kernel_size\": 7,\n",
      "    \"model_type\": \"encodec\",\n",
      "    \"norm_type\": \"weight_norm\",\n",
      "    \"normalize\": false,\n",
      "    \"num_filters\": 64,\n",
      "    \"num_lstm_layers\": 2,\n",
      "    \"num_residual_layers\": 1,\n",
      "    \"overlap\": null,\n",
      "    \"pad_mode\": \"reflect\",\n",
      "    \"residual_kernel_size\": 3,\n",
      "    \"sampling_rate\": 32000,\n",
      "    \"target_bandwidths\": [\n",
      "      2.2\n",
      "    ],\n",
      "    \"torch_dtype\": \"float32\",\n",
      "    \"trim_right_ratio\": 1.0,\n",
      "    \"upsampling_ratios\": [\n",
      "      8,\n",
      "      5,\n",
      "      4,\n",
      "      4\n",
      "    ],\n",
      "    \"use_causal_conv\": false,\n",
      "    \"use_conv_shortcut\": false\n",
      "  },\n",
      "  \"decoder\": {\n",
      "    \"activation_dropout\": 0.0,\n",
      "    \"activation_function\": \"gelu\",\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"audio_channels\": 1,\n",
      "    \"classifier_dropout\": 0.0,\n",
      "    \"decoder_start_token_id\": 2048,\n",
      "    \"dropout\": 0.1,\n",
      "    \"ffn_dim\": 8192,\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_factor\": 0.02,\n",
      "    \"layerdrop\": 0.0,\n",
      "    \"max_position_embeddings\": 2048,\n",
      "    \"model_type\": \"musicgen_decoder\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_codebooks\": 4,\n",
      "    \"num_hidden_layers\": 48,\n",
      "    \"scale_embedding\": false,\n",
      "    \"torch_dtype\": \"float32\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 2048\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"model_type\": \"musicgen\",\n",
      "  \"text_encoder\": {\n",
      "    \"_name_or_path\": \"t5-base\",\n",
      "    \"architectures\": [\n",
      "      \"T5ForConditionalGeneration\"\n",
      "    ],\n",
      "    \"classifier_dropout\": 0.0,\n",
      "    \"d_ff\": 3072,\n",
      "    \"d_kv\": 64,\n",
      "    \"d_model\": 768,\n",
      "    \"decoder_start_token_id\": 0,\n",
      "    \"dense_act_fn\": \"relu\",\n",
      "    \"dropout_rate\": 0.1,\n",
      "    \"feed_forward_proj\": \"relu\",\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"is_gated_act\": false,\n",
      "    \"layer_norm_epsilon\": 1e-06,\n",
      "    \"model_type\": \"t5\",\n",
      "    \"n_positions\": 512,\n",
      "    \"num_decoder_layers\": 12,\n",
      "    \"num_heads\": 12,\n",
      "    \"num_layers\": 12,\n",
      "    \"output_past\": true,\n",
      "    \"relative_attention_max_distance\": 128,\n",
      "    \"relative_attention_num_buckets\": 32,\n",
      "    \"task_specific_params\": {\n",
      "      \"summarization\": {\n",
      "        \"early_stopping\": true,\n",
      "        \"length_penalty\": 2.0,\n",
      "        \"max_length\": 200,\n",
      "        \"min_length\": 30,\n",
      "        \"no_repeat_ngram_size\": 3,\n",
      "        \"num_beams\": 4,\n",
      "        \"prefix\": \"summarize: \"\n",
      "      },\n",
      "      \"translation_en_to_de\": {\n",
      "        \"early_stopping\": true,\n",
      "        \"max_length\": 300,\n",
      "        \"num_beams\": 4,\n",
      "        \"prefix\": \"translate English to German: \"\n",
      "      },\n",
      "      \"translation_en_to_fr\": {\n",
      "        \"early_stopping\": true,\n",
      "        \"max_length\": 300,\n",
      "        \"num_beams\": 4,\n",
      "        \"prefix\": \"translate English to French: \"\n",
      "      },\n",
      "      \"translation_en_to_ro\": {\n",
      "        \"early_stopping\": true,\n",
      "        \"max_length\": 300,\n",
      "        \"num_beams\": 4,\n",
      "        \"prefix\": \"translate English to Romanian: \"\n",
      "      }\n",
      "    },\n",
      "    \"torch_dtype\": \"float32\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 32128\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.3\"\n",
      "}\n",
      "\n",
      "EncodecConfig {\n",
      "  \"architectures\": [\n",
      "    \"EncodecModel\"\n",
      "  ],\n",
      "  \"audio_channels\": 1,\n",
      "  \"chunk_length_s\": null,\n",
      "  \"codebook_dim\": 128,\n",
      "  \"codebook_size\": 2048,\n",
      "  \"compress\": 2,\n",
      "  \"dilation_growth_rate\": 2,\n",
      "  \"hidden_size\": 128,\n",
      "  \"kernel_size\": 7,\n",
      "  \"last_kernel_size\": 7,\n",
      "  \"model_type\": \"encodec\",\n",
      "  \"norm_type\": \"weight_norm\",\n",
      "  \"normalize\": false,\n",
      "  \"num_filters\": 64,\n",
      "  \"num_lstm_layers\": 2,\n",
      "  \"num_residual_layers\": 1,\n",
      "  \"overlap\": null,\n",
      "  \"pad_mode\": \"reflect\",\n",
      "  \"residual_kernel_size\": 3,\n",
      "  \"sampling_rate\": 32000,\n",
      "  \"target_bandwidths\": [\n",
      "    2.2\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"trim_right_ratio\": 1.0,\n",
      "  \"upsampling_ratios\": [\n",
      "    8,\n",
      "    5,\n",
      "    4,\n",
      "    4\n",
      "  ],\n",
      "  \"use_causal_conv\": false,\n",
      "  \"use_conv_shortcut\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MusicgenForConditionalGeneration(\n",
       "  (text_encoder): T5EncoderModel(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder): EncodecModel(\n",
       "    (encoder): EncodecEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): EncodecConv1d(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            1, 64, kernel_size=(7,), stride=(1,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): EncodecResnetBlock(\n",
       "          (block): ModuleList(\n",
       "            (0): ELU(alpha=1.0)\n",
       "            (1): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                64, 32, kernel_size=(3,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ELU(alpha=1.0)\n",
       "            (3): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                32, 64, kernel_size=(1,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "        )\n",
       "        (2): ELU(alpha=1.0)\n",
       "        (3): EncodecConv1d(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            64, 128, kernel_size=(8,), stride=(4,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): EncodecResnetBlock(\n",
       "          (block): ModuleList(\n",
       "            (0): ELU(alpha=1.0)\n",
       "            (1): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                128, 64, kernel_size=(3,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ELU(alpha=1.0)\n",
       "            (3): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                64, 128, kernel_size=(1,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "        )\n",
       "        (5): ELU(alpha=1.0)\n",
       "        (6): EncodecConv1d(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            128, 256, kernel_size=(8,), stride=(4,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): EncodecResnetBlock(\n",
       "          (block): ModuleList(\n",
       "            (0): ELU(alpha=1.0)\n",
       "            (1): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                256, 128, kernel_size=(3,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ELU(alpha=1.0)\n",
       "            (3): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                128, 256, kernel_size=(1,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "        )\n",
       "        (8): ELU(alpha=1.0)\n",
       "        (9): EncodecConv1d(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            256, 512, kernel_size=(10,), stride=(5,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): EncodecResnetBlock(\n",
       "          (block): ModuleList(\n",
       "            (0): ELU(alpha=1.0)\n",
       "            (1): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                512, 256, kernel_size=(3,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ELU(alpha=1.0)\n",
       "            (3): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                256, 512, kernel_size=(1,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "        )\n",
       "        (11): ELU(alpha=1.0)\n",
       "        (12): EncodecConv1d(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            512, 1024, kernel_size=(16,), stride=(8,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (13): EncodecLSTM(\n",
       "          (lstm): LSTM(1024, 1024, num_layers=2)\n",
       "        )\n",
       "        (14): ELU(alpha=1.0)\n",
       "        (15): EncodecConv1d(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            1024, 128, kernel_size=(7,), stride=(1,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): EncodecDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): EncodecConv1d(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            128, 1024, kernel_size=(7,), stride=(1,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): EncodecLSTM(\n",
       "          (lstm): LSTM(1024, 1024, num_layers=2)\n",
       "        )\n",
       "        (2): ELU(alpha=1.0)\n",
       "        (3): EncodecConvTranspose1d(\n",
       "          (conv): ParametrizedConvTranspose1d(\n",
       "            1024, 512, kernel_size=(16,), stride=(8,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): EncodecResnetBlock(\n",
       "          (block): ModuleList(\n",
       "            (0): ELU(alpha=1.0)\n",
       "            (1): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                512, 256, kernel_size=(3,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ELU(alpha=1.0)\n",
       "            (3): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                256, 512, kernel_size=(1,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "        )\n",
       "        (5): ELU(alpha=1.0)\n",
       "        (6): EncodecConvTranspose1d(\n",
       "          (conv): ParametrizedConvTranspose1d(\n",
       "            512, 256, kernel_size=(10,), stride=(5,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): EncodecResnetBlock(\n",
       "          (block): ModuleList(\n",
       "            (0): ELU(alpha=1.0)\n",
       "            (1): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                256, 128, kernel_size=(3,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ELU(alpha=1.0)\n",
       "            (3): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                128, 256, kernel_size=(1,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "        )\n",
       "        (8): ELU(alpha=1.0)\n",
       "        (9): EncodecConvTranspose1d(\n",
       "          (conv): ParametrizedConvTranspose1d(\n",
       "            256, 128, kernel_size=(8,), stride=(4,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): EncodecResnetBlock(\n",
       "          (block): ModuleList(\n",
       "            (0): ELU(alpha=1.0)\n",
       "            (1): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                128, 64, kernel_size=(3,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ELU(alpha=1.0)\n",
       "            (3): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                64, 128, kernel_size=(1,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "        )\n",
       "        (11): ELU(alpha=1.0)\n",
       "        (12): EncodecConvTranspose1d(\n",
       "          (conv): ParametrizedConvTranspose1d(\n",
       "            128, 64, kernel_size=(8,), stride=(4,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (13): EncodecResnetBlock(\n",
       "          (block): ModuleList(\n",
       "            (0): ELU(alpha=1.0)\n",
       "            (1): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                64, 32, kernel_size=(3,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): ELU(alpha=1.0)\n",
       "            (3): EncodecConv1d(\n",
       "              (conv): ParametrizedConv1d(\n",
       "                32, 64, kernel_size=(1,), stride=(1,)\n",
       "                (parametrizations): ModuleDict(\n",
       "                  (weight): ParametrizationList(\n",
       "                    (0): _WeightNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "        )\n",
       "        (14): ELU(alpha=1.0)\n",
       "        (15): EncodecConv1d(\n",
       "          (conv): ParametrizedConv1d(\n",
       "            64, 1, kernel_size=(7,), stride=(1,)\n",
       "            (parametrizations): ModuleDict(\n",
       "              (weight): ParametrizationList(\n",
       "                (0): _WeightNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (quantizer): EncodecResidualVectorQuantizer(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x EncodecVectorQuantization(\n",
       "          (codebook): EncodecEuclideanCodebook()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): MusicgenForCausalLM(\n",
       "    (model): MusicgenModel(\n",
       "      (decoder): MusicgenDecoder(\n",
       "        (embed_tokens): ModuleList(\n",
       "          (0-3): 4 x Embedding(2049, 2048)\n",
       "        )\n",
       "        (embed_positions): MusicgenSinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0-47): 48 x MusicgenDecoderLayer(\n",
       "            (self_attn): MusicgenSdpaAttention(\n",
       "              (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MusicgenSdpaAttention(\n",
       "              (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (fc2): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_heads): ModuleList(\n",
       "      (0-3): 4 x Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (enc_to_dec_proj): Linear(in_features=768, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, MusicgenForConditionalGeneration, AutoProcessor, EncodecModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# 这个模型是我们要训练的\n",
    "model_name = \"/root/autodl-tmp/musicgen-large\"  # 可选：small, medium, large\n",
    "# 初次使用记得去掉local_files_only=True\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "processor = AutoProcessor.from_pretrained(model_name, local_files_only=True)\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(model_name, local_files_only=True).half().to(device)\n",
    "# model.half()解决精度问题报错\n",
    "\n",
    "model.config.decoder.decoder_start_token_id = model.generation_config.decoder_start_token_id\n",
    "print(\"decoder_start_token_id:\", model.config.decoder.decoder_start_token_id)\n",
    "print(model.config)\n",
    "\n",
    "# 这个模型用于将音频转化为 tokens\n",
    "encodec_model_name = \"/root/autodl-tmp/encodec_32khz\"\n",
    "encodec_model = EncodecModel.from_pretrained(encodec_model_name, local_files_only=True).to(device)\n",
    "encodec_model.eval()\n",
    "print(encodec_model.config)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def process_and_expand_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Process dataset by expanding each audio into multiple 10-second slices\n",
    "    \"\"\"\n",
    "    all_slices = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    print(\"Processing and slicing audio files...\")\n",
    "    \n",
    "    for i, record in enumerate(dataset['train']):\n",
    "        audio_array = record['audio']['array']\n",
    "        sampling_rate = record['audio']['sampling_rate']\n",
    "        \n",
    "        # Resample to 32kHz if needed\n",
    "        if sampling_rate != 32000:\n",
    "            audio_array = librosa.resample(audio_array, orig_sr=sampling_rate, target_sr=32000)\n",
    "            sampling_rate = 32000\n",
    "        \n",
    "        # Calculate 10-second slice parameters\n",
    "        slice_duration = 30.0\n",
    "        slice_length = int(slice_duration * sampling_rate)  # 320,000 samples\n",
    "        audio_length = len(audio_array)\n",
    "        num_slices = audio_length // slice_length\n",
    "        \n",
    "        # Skip audio shorter than 10 seconds\n",
    "        if num_slices == 0:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Enhanced text processing for Chinese National Pentatonic Mode Dataset\n",
    "        # Define label mappings\n",
    "        system_labels = ['C', '#C/bD', 'D', '#D/bE', 'E', 'F', '#F/bG', 'G', '#G/bA', 'A', '#A/bB', 'B']\n",
    "        tonic_labels = ['C', '#C/bD', 'D', '#D/bE', 'E', 'F', '#F/bG', 'G', '#G/bA', 'A', '#A/bB', 'B']\n",
    "        pattern_labels = ['Gong', 'Shang', 'Jue', 'Zhi', 'Yu']\n",
    "        type_labels = ['Pentatonic', 'Hexatonic_Qingjue', 'Hexatonic_Biangong', 'Heptatonic_Yayue', 'Heptatonic_Qingyue', 'Heptatonic_Yanyue']\n",
    "        \n",
    "        # Extract musical attributes\n",
    "        system = system_labels[record['system']]\n",
    "        tonic = tonic_labels[record['tonic']]\n",
    "        pattern = pattern_labels[record['pattern']]\n",
    "        type_val = type_labels[record['type']]\n",
    "        \n",
    "        # Create rich, natural language description for MusicGen\n",
    "        text = f\"Chinese traditional music in {pattern} pattern, {type_val.replace('_', ' ').lower()} scale, key of {tonic}\"\n",
    "        # print(f\"Sample text description: {text}\") \n",
    "        text_tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Process each complete 10-second slice\n",
    "        for slice_idx in range(num_slices):\n",
    "            start_idx = slice_idx * slice_length\n",
    "            end_idx = start_idx + slice_length\n",
    "            \n",
    "            # Extract slice\n",
    "            audio_slice = audio_array[start_idx:end_idx]\n",
    "            \n",
    "            # Encode on GPU\n",
    "            audio_tensor = torch.from_numpy(audio_slice).float().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                audio_tensor = audio_tensor.to(device)\n",
    "                encoded_audio = encodec_model.encode(audio_tensor)\n",
    "                audio_tokens = encoded_audio.audio_codes.squeeze(0).squeeze(0).transpose(0, 1).cpu()\n",
    "                \n",
    "                # Clean up GPU memory\n",
    "                del audio_tensor, encoded_audio\n",
    "            \n",
    "            # Create slice record\n",
    "            slice_record = {\n",
    "                \"input_ids\": text_tokens[\"input_ids\"].squeeze(0).clone(),\n",
    "                \"attention_mask\": text_tokens[\"attention_mask\"].squeeze(0).clone(),\n",
    "                \"labels\": audio_tokens,\n",
    "                \"original_index\": i,\n",
    "                \"slice_index\": slice_idx,\n",
    "                \"total_slices\": num_slices\n",
    "            }\n",
    "            \n",
    "            all_slices.append(slice_record)\n",
    "        \n",
    "        # Progress tracking\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(dataset['train'])} files, \"\n",
    "                  f\"generated {len(all_slices)} slices so far\")\n",
    "            torch.cuda.empty_cache()  # Clean up periodically\n",
    "    \n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"Original files: {len(dataset['train'])}\")\n",
    "    print(f\"Skipped files: {skipped_count}\")\n",
    "    print(f\"Total slices generated: {len(all_slices)}\")\n",
    "    \n",
    "    # Convert to HuggingFace dataset format\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Organize data by columns\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [slice_rec[\"input_ids\"] for slice_rec in all_slices],\n",
    "        \"attention_mask\": [slice_rec[\"attention_mask\"] for slice_rec in all_slices],\n",
    "        \"labels\": [slice_rec[\"labels\"] for slice_rec in all_slices],\n",
    "        \"original_index\": [slice_rec[\"original_index\"] for slice_rec in all_slices],\n",
    "        \"slice_index\": [slice_rec[\"slice_index\"] for slice_rec in all_slices],\n",
    "        \"total_slices\": [slice_rec[\"total_slices\"] for slice_rec in all_slices]\n",
    "    }\n",
    "    \n",
    "    # Create new dataset\n",
    "    sliced_dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    return sliced_dataset\n",
    "\n",
    "def my_collator(batch):\n",
    "    # Extract data from batch\n",
    "    input_ids_list = [torch.tensor(item[\"input_ids\"], dtype=torch.long) for item in batch]\n",
    "    labels_list = [torch.tensor(item[\"labels\"], dtype=torch.long) for item in batch]\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=0)\n",
    "    mask = (input_ids != 0).bool()\n",
    "    labels = torch.stack(labels_list, dim=0)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and slicing audio files...\n",
      "Processed 10/287 files, generated 153 slices so far\n",
      "Processed 20/287 files, generated 297 slices so far\n",
      "Processed 30/287 files, generated 495 slices so far\n",
      "Processed 40/287 files, generated 645 slices so far\n",
      "Processed 50/287 files, generated 784 slices so far\n",
      "Processed 60/287 files, generated 1008 slices so far\n",
      "Processed 70/287 files, generated 1169 slices so far\n",
      "Processed 80/287 files, generated 1354 slices so far\n",
      "Processed 90/287 files, generated 1545 slices so far\n",
      "Processed 100/287 files, generated 1716 slices so far\n",
      "Processed 110/287 files, generated 1869 slices so far\n",
      "Processed 120/287 files, generated 2028 slices so far\n",
      "Processed 130/287 files, generated 2175 slices so far\n",
      "Processed 140/287 files, generated 2361 slices so far\n",
      "Processed 150/287 files, generated 2552 slices so far\n",
      "Processed 160/287 files, generated 2735 slices so far\n",
      "Processed 170/287 files, generated 2863 slices so far\n",
      "Processed 180/287 files, generated 3021 slices so far\n",
      "Processed 190/287 files, generated 3249 slices so far\n",
      "Processed 200/287 files, generated 3367 slices so far\n",
      "Processed 210/287 files, generated 3523 slices so far\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = \"/root/autodl-tmp/CNPM\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Process and expand dataset\n",
    "dataset_sliced = process_and_expand_dataset(dataset)\n",
    "\n",
    "print(f\"Final sliced dataset size: {len(dataset_sliced)}\")\n",
    "if len(dataset['train']) > 0:\n",
    "    print(f\"Average slices per original audio: {len(dataset_sliced) / len(dataset['train']):.1f}\")\n",
    "\n",
    "# Wrap in DatasetDict format to match original structure\n",
    "from datasets import DatasetDict\n",
    "dataset_sliced = DatasetDict({\"train\": dataset_sliced})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置lora参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up existing LoRA model.\n",
      "LoRA model device: cuda:0\n",
      "trainable params: 6,291,456 || all params: 3,429,761,602 || trainable%: 0.1834\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import gc\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                          # LoRA rank\n",
    "    lora_alpha=32,                # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],  # Target attention modules\n",
    "    lora_dropout=0.05,            # Dropout rate\n",
    "    bias=\"none\",                  # Don't adjust bias\n",
    "    task_type=\"CAUSAL_LM\",        # Causal language modeling\n",
    ")\n",
    "\n",
    "# Clean up any existing LoRA model\n",
    "try:\n",
    "    if 'lora_model' in globals():\n",
    "        lora_model.unload()\n",
    "        del lora_model\n",
    "    if hasattr(model, 'peft_config'):\n",
    "        delattr(model, 'peft_config')\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Cleaned up existing LoRA model.\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing LoRA model to clean up: {e}\")\n",
    "\n",
    "# Apply LoRA and explicitly move to GPU (safe regardless of base model device)\n",
    "lora_model = get_peft_model(model, lora_config).to(device)\n",
    "print(f\"LoRA model device: {next(lora_model.parameters()).device}\")\n",
    "\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Save initial LoRA weights\n",
    "lora_model.save_pretrained(\"./outputs/musicgen-lora/initial_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjelly-zhao-42\u001b[0m (\u001b[33mjelly-zhao-42-peking-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "WANDB_API_KEY = \"your_wandb_api_key_here\"  # Replace with your actual WandB API key\n",
    "\n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/MusicGen/wandb/run-20250526_181420-piv4o58y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jelly-zhao-42-peking-university/musicgen-chinese-pentatonic/runs/piv4o58y' target=\"_blank\">lora-finetune-20250526_1814</a></strong> to <a href='https://wandb.ai/jelly-zhao-42-peking-university/musicgen-chinese-pentatonic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jelly-zhao-42-peking-university/musicgen-chinese-pentatonic' target=\"_blank\">https://wandb.ai/jelly-zhao-42-peking-university/musicgen-chinese-pentatonic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jelly-zhao-42-peking-university/musicgen-chinese-pentatonic/runs/piv4o58y' target=\"_blank\">https://wandb.ai/jelly-zhao-42-peking-university/musicgen-chinese-pentatonic/runs/piv4o58y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize wandb tracking\n",
    "wandb.init(\n",
    "    project=\"musicgen-chinese-pentatonic\",\n",
    "    name=f\"lora-finetune-{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "    config={\n",
    "        \"model\": \"musicgen-large\",\n",
    "        \"lora_rank\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"num_epochs\": 5,\n",
    "        \"dataset\": \"Chinese National Pentatonic Mode\",\n",
    "        \"gpu\": \"A100-80GB\"\n",
    "    },\n",
    "    tags=[\"musicgen\", \"lora\", \"chinese-music\"],\n",
    ")\n",
    "\n",
    "# High-performance training arguments with wandb monitoring\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs/musicgen-lora\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=2,\n",
    "    logging_steps=10,                    # Frequent logging for good plots\n",
    "    save_steps=500,\n",
    "    eval_steps=250,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # WANDB Integration\n",
    "    report_to=\"wandb\",                   # Enable wandb logging\n",
    "    logging_dir=\"./logs\",\n",
    "    \n",
    "    remove_unused_columns=False,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    "    dataloader_prefetch_factor=2,\n",
    "    group_by_length=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "dataset_sliced = dataset_sliced[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Memory monitoring function with target usage\n",
    "def print_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB, Total: {total:.2f}GB\")\n",
    "        print(f\"Utilization: {allocated/total*100:.1f}% (Target: 60-70%)\")\n",
    "\n",
    "# Enhanced trainer with memory monitoring and aggressive optimization\n",
    "class HighPerformanceTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.step_count = 0\n",
    "\n",
    "    def training_step(\n",
    "        self, model, inputs, num_items_in_batch=None\n",
    "    ):\n",
    "        \"\"\"Optimized training step for high throughput\"\"\"\n",
    "        model.train()\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        # Forward pass with autocast for mixed precision\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
    "        \n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        self.accelerator.backward(loss)\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Less frequent memory cleanup for better performance\n",
    "        if self.step_count % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return loss.detach()\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"Optimized prediction step\"\"\"\n",
    "        result = super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n",
    "        \n",
    "        # Minimal cleanup during evaluation\n",
    "        if torch.cuda.memory_allocated() > 50 * 1024**3:  # Only if using >50GB\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Initialize trainer with proper label configuration\n",
    "trainer = HighPerformanceTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_sliced[\"train\"],\n",
    "    eval_dataset=dataset_sliced[\"test\"],\n",
    "    data_collator=my_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage:\n",
      "GPU Memory - Allocated: 6.65GB, Reserved: 7.46GB, Total: 79.14GB\n",
      "Utilization: 8.4% (Target: 60-70%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 18:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>6.357000</td>\n",
       "      <td>6.331653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-performance training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.musicgen.modeling_musicgen import MusicgenSinusoidalPositionalEmbedding\n",
    "\n",
    "# Add the offset property to the class\n",
    "MusicgenSinusoidalPositionalEmbedding.offset = 0  # or 2, depending on your needs\n",
    "\n",
    "# Train the model with high performance settings\n",
    "print(\"Initial memory usage:\")\n",
    "print_memory_usage()\n",
    "trainer.train()\n",
    "print(\"High-performance training completed successfully!\")\n",
    "    \n",
    "# Save final model\n",
    "trainer.save_model(\"./outputs/musicgen-lora/final_model\")\n",
    "\n",
    "wandb.finish()\n",
    "    \n",
    "# Final memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
